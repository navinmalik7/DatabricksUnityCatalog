{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "157c1d4a-8e69-4521-ae57-ceebc90b0af6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.databricksdevfiles.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.databricksdevfiles.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.databricksdevfiles.dfs.core.windows.net\", \"a16fee0e-d0cf-4509-ac98-68a5a070213d\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.databricksdevfiles.dfs.core.windows.net\", \"4Bq8Q~atSC96MQV0ColTWrG.C6AdHVGFBKPf4ahL\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.databricksdevfiles.dfs.core.windows.net\", \"https://login.microsoftonline.com/033f9491-2edf-40df-8787-0db5a02feaa3/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248412f4-0b65-4077-add2-5abae26f462c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "source = \"abfss://test@databricksdevfiles.dfs.core.windows.net/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffb60344-ecaa-41cf-b51e-5b3ae1923e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"parquet\").load(f'{source}/parquetfolder/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1074da4a-e2ba-4add-8792-4bae35cfd4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7ba889-f9fc-4795-a1a4-ef453b9862e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").format(\"delta\").saveAsTable('`delta`.DeltaSpark')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "4. Creating Delta Tables using PySpark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
